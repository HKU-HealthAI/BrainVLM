## A Foundation Model for Presurgical Brain Tumor Diagnosis and MRI Interpretation
Yinong Wang â€ , Jianwen Chen â€ , Zhou Chen â€ , Shuwen Kuang â€ , Haoning Jiang, Yanzhao Shi, Huichun Yuan, Yan-ran (Joyce) Wang, Bing Wang, Lei Wu, Bin Tang, Li Meng, Baihua Luo, Bin Zhou, Wei Ding, Weiming Zhong, Wei Hou, Yuanbing Chen, Zhiping Wan, Wei Wang, Zhenkun Xiao, Wenwu Wan, Allen He, Yuyin Zhou, Longbo Zhang, Feifei Wang, Zhixiong Liu, Michael Iv, Xuan Gong*, Liangqiong Qu*

â€  These authors contributed equally to this work.

*Corresponding authors.

BrainVLM is a foundation model designed for comprehensive brain tumor analysis. It leverages multi-parametric MRI scans (T1, T1c, T2, and FLAIR sequences) and _optional_ patient metadata as input, and provides diagnosis (among 12 brain tumor categories of WHO-CNS5) and radiology report for patient.

ðŸ”¥ðŸ”¥ðŸ”¥ We have released our evaluation code and checkpoints for brain tumor diagnosis and radiology report generation. Welcome everyone to try it now!

## Table of Contents
[Installation](#Installation)

[Prepare model weights](#Prepare_model_weights)

[Prepare test data](#Prepare_test_data)

[Evaluation](#Evaluation)

[Example Output](#Example_Output)

## Environment Installation 
Linux Ubuntu 22.04.5
1. Clone this repository and navigate to the brainvlm folder
~~~~
git clone https://github.com/HKU-HealthAI/BrainVLM.git
cd BrainVLM
conda env create -f environment.yml
~~~~

## Prepare model weights
BrainVLM comprises three key components: (1) a vision encoder for extracting image features, (2) a MLP projector that integrates the vision encoder with the LLM, and (3) a LLM backbone based on LLaMA 3.1-8B. To utilize BrainVLM, we need to load the vision encoder, LLM backbone, and fine-tuned BrainVLM checkpoint.

| Model                  | Description                          | Download Link                                      | Load Instructions                              |
|------------------------|--------------------------------------|----------------------------------------------------|------------------------------------------------|
| Llama3.1-8B Instruct   | Backbone model of BrainVLM | [huggingface](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) | Load in minigpt4/configs/models/minigpt4_vicuna0.yaml: line 18, "llama_model: " |
| BiomedCLIP             | Vision Encoder | [huggingface](https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224) | BrainVLM automatically loads this checkpoint, no path needed |
| BrainVLM (Diagnosis and Report) | Checkpoints for report and diagnosis | [google drive](https://drive.google.com/file/d/16yiqIvVVOANpI7OoxBKXvx5NPy0c625n/view?usp=drive_link) | See section [Evaluation](#Evaluation)                                       |

## Prepare test data files:

We provide a sample JSON file for testing: [example_test.json](./example_test.json), which includes data for two patients (patient1 and patient2). BrainVLM supports .nii.gz and .npy file formats as input.

### 1. Test data example:
For [patient1](examples/patient1/), it includes 6 MRI sequences stored in the directory structure shown below. Each file corresponds to a specific MRI modality.
```
This is original paitent MRI sequences
/examples
   â””â”€â”€/patient1
      â”œâ”€â”€patient1_sag t1c+.nii.gz
      â”œâ”€â”€patient1_cor t1c+.nii.gz
      â”œâ”€â”€patient1_ax t1.nii.gz
      â”œâ”€â”€patient1_ax t2.nii.gz
      â”œâ”€â”€patient1_ax t2f.nii.gz
      â””â”€â”€patient1_ax t1c+.nii.gz  
```
__Input requirement:__ BrainVLM requires three inputs: 1) a list up to 5 MRI sequences, 2) MRI modality information, and 3) patient metadata (age and gender). It uses five core 3D MRI sequences as visual input: T1, T1c (same view as T1), T2, FLAIR, and an additional T1c (different view). 


__Combination Construction:__ BrainVLM utilized five core 3D MRI sequences as visual inputâ€”T1 (axial or another view), T1c from the same view as T1, T2 (axial or another view), FLAIR (axial or another view), and an additional T1c from a different view than T1.


__Inference:__ During inference, BrainVLM organizes MRI combinations based on the construction rule, with the final diagnosis determined by the most frequent prediction. 

To run inference, a test JSON file in the same format as [example_test.json](./example_test.json) is required.

### 2. Origanizing test json file.
For patient1, it has 3 parts: 1) image_list; 2) modality_list; 3) patient metadata (_optional_).
#### 1) image_list
The image_list defines combinations of MRI sequences for inference. For patient1, available modalities include axial T1, T2, FLAIR, and T1c+, plus coronal and sagittal T1c+. Two combinations are created based on the construction rule, each containing paths to five .nii.gz files.

#### 2) modality_list
The modality_list specifies the modality names corresponding to each file path in the image_list combinations.

```
/patient1
   â””â”€â”€ /image_list
       â””â”€â”€ /combination_1
           â”œâ”€â”€ patient1_ax t1.nii.gz
           â”œâ”€â”€ patient1_ax t1c+.nii.gz
           â”œâ”€â”€ patient1_ax t2.nii.gz
           â”œâ”€â”€ patient1_ax t2f.nii.gz
           â””â”€â”€ patient1_cor t1c+.nii.gz
        â””â”€â”€/combination_2
           â”œâ”€â”€ patient1_ax t1.nii.gz
           â”œâ”€â”€ patient1_ax t1c+.nii.gz
           â”œâ”€â”€ patient1_ax t2.nii.gz
           â”œâ”€â”€ patient1_ax t2f.nii.gz
           â””â”€â”€ patient1_sag t1c+.nii.gz
   â””â”€â”€/modality_list
         â””â”€â”€ /combination_1
            â”œâ”€â”€ ax t1
            â”œâ”€â”€ ax t1c+
            â”œâ”€â”€ ax t2
            â”œâ”€â”€ ax t2f
            â””â”€â”€ cor t1c+
         â””â”€â”€/combination_2
            â”œâ”€â”€ ax t1
            â”œâ”€â”€ ax t1c+
            â”œâ”€â”€ ax t2
            â”œâ”€â”€ ax t2f
            â””â”€â”€ cor t1c+
   â””â”€â”€patient meta data
         â”œâ”€â”€ Age
         â””â”€â”€ Gender
```
For custom data testing, you need organize the test combinations same with patient1. 

### 3. Testing for patient with incomplete data.
For a patient with incomplete data, such as [patient2](./examples/patient2/), only the following MRI sequences are available: axial T1, T1c+, and T2, along with coronal T1c+ and sagittal T1c+. The axial FLAIR (T2f) sequence is missing. Additionally, patient2 lacks gender and age information.
```
This is original paitent MRI sequences
/examples
   â””â”€â”€/patient2
      â”œâ”€â”€patient2_sag t1c+.nii.gz
      â”œâ”€â”€patient2_cor t1c+.nii.gz
      â”œâ”€â”€patient2_ax t1.nii.gz
      â”œâ”€â”€patient2_ax t2.nii.gz
      â””â”€â”€patient2_ax t1c+.nii.gz  
```


For testing patient2, we can create a combination by keeping the other modalities fixed and substituting coronal T1c+ for the missing axial FLAIR. The resulting json data is shown below:
```
/patient2
   â””â”€â”€ /image_list
       â””â”€â”€ /combination_1
           â”œâ”€â”€ patient2_ax t1.nii.gz
           â”œâ”€â”€ patient2_ax t1c+.nii.gz
           â”œâ”€â”€ patient2_ax t2.nii.gz
           â”œâ”€â”€ patient2_cor t1c+.nii.gz
           â””â”€â”€ patient2_sag t1c+.nii.gz
   â””â”€â”€/modality_list
         â””â”€â”€ /combination_1
            â”œâ”€â”€ ax t1
            â”œâ”€â”€ ax t1c+
            â”œâ”€â”€ ax t2
            â”œâ”€â”€ cor t1c+
            â””â”€â”€ sag t1c+

```

## Evaluation

The [eval.py](./eval.py) is utilized for diagnosis and report generation, supporting brain tumor classification and radiology report generation through this command.

```
python eval.py --test_json_path <path_to_json> --model_ckpt_path <path_to_checkpoint>
```
Replace <path_to_json> with the path to your test JSON file and <path_to_checkpoint> with the BrainVLM checkpoint path. For example, if the BrainVLM checkpoint is downloaded to ./ckpts/checkpoint_1.pth, run:

```
python eval.py example_test.json ./ckpts/checkpoint_1.pth
```

## Example Output
#### For patient 1:
```
combination modality: ['ax t1', 'ax t1c+', 'ax t2', 'ax t2f', 'cor t1c+']

combination_0: In Right cerebellum, there is a mass lesion with hypointense in T1, hyperintense in T2. After contrast administration, there is a heterogeneous enhancement. Compression of the fourth ventricle is observed. No midline structure shift. This patient was diagnosed with cranial and paraspinal nerve tumour. 
```

```
combination modality: ['ax t1', 'ax t1c+', 'ax t2', 'ax t2f', 'sag t1c+']

combination_1: In Right cerebellum, there is a mass lesion with hypointense in T1, hyperintense in T2, hyperintense in FLAIR. There is a hypointense T1 signal, hyperintense T2 signal, hyperintense FLAIR signal in Supratentorial white matter. After contrast administration, there is a marked heterogeneous enhancement. Compression of the fourth ventricle is observed. No midline structure shift. This patient was diagnosed with cranial and paraspinal nerve tumour. 
```

```
Final diagnosis:  This patient was diagnosed with cranial and paraspinal nerve tumour
```


## Acknowledgement
Upon acceptance of the paper, we will release all models' weights and relevant source code for pretraining, fine-tuning, and uncertainty qualification for formal testing to facilitate research transparency and community collaboration.

Besides, we will build a online website for efficientlly and automatically brain tumor diagnosis testing.

Our code builds upon MiniGPT-4 and utilizes checkpoints based on LLaMA 3.1 and BiomedCLIP. We would like to thank them.
